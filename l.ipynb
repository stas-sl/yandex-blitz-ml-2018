{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L. Wi-Fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from collections import defaultdict\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates distance between 2 points\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "# computes how many ngrams string s1 ans s2 have in common\n",
    "def compare_ngrams(s1, s2, n):\n",
    "    ngrams1 = [s1[i:i + n] for i in range(len(s1) - n)]\n",
    "    ngrams2 = [s2[i:i + n] for i in range(len(s2) - n)]\n",
    "    count = 0\n",
    "    for ngram1 in ngrams1:\n",
    "        for ngram2 in ngrams2:\n",
    "            if ngram1 == ngram2:\n",
    "                count += 1\n",
    "    return count / max(len(s1), len(s2))\n",
    "\n",
    "def compute_score(y_hat, y_true):\n",
    "    groups = df.iloc[train_idx].group_num if y_true.params['train'] else df.iloc[test_idx].group_num\n",
    "    groups = groups.values\n",
    "    targets = y_true.get_label().values\n",
    "    max_pred = defaultdict(lambda: (-1, -1))\n",
    "    n = len(y_hat)\n",
    "    for i in range(n):\n",
    "        if max_pred[groups[i]][0] < y_hat[i]:\n",
    "            max_pred[groups[i]] = (y_hat[i], i)\n",
    "    acc = 0\n",
    "    for _, i in max_pred.values():\n",
    "        acc += targets[i]\n",
    "    acc /= len(max_pred)\n",
    "    return 'score', 1 + 6 * (acc - 0.5), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>candidate_num</th>\n",
       "      <th>group_num</th>\n",
       "      <th>has_wifi</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>names</th>\n",
       "      <th>publishing_status</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>ssid</th>\n",
       "      <th>target</th>\n",
       "      <th>urls</th>\n",
       "      <th>user_lat</th>\n",
       "      <th>user_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–†–æ—Å—Å–∏—è, –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –û–±—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –ú...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.007759</td>\n",
       "      <td>82.667023</td>\n",
       "      <td>[\"–°–±–µ—Ä–±–∞–Ω–∫ –†–æ—Å—Å–∏–∏, –±–∞–Ω–∫–æ–º–∞—Ç\", \"Sberbank Rossii...</td>\n",
       "      <td>publish</td>\n",
       "      <td>[30336]</td>\n",
       "      <td>Tolmachevo-MTS-Free</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"type\": \"main\", \"value\": \"http://www.sberban...</td>\n",
       "      <td>55.007579</td>\n",
       "      <td>82.666723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–†–æ—Å—Å–∏—è, –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –∞—ç—Ä–æ–ø–æ—Ä—Ç –ù–æ–≤–æ—Å–∏...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.007879</td>\n",
       "      <td>82.665401</td>\n",
       "      <td>[\"–ò–ª-86\"]</td>\n",
       "      <td>publish</td>\n",
       "      <td>[3481524327]</td>\n",
       "      <td>Tolmachevo-MTS-Free</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>55.007579</td>\n",
       "      <td>82.666723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–†–æ—Å—Å–∏—è, –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –û–±—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –ú...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.007570</td>\n",
       "      <td>82.667069</td>\n",
       "      <td>[\"–ß–∞—à–∫–∞ –∫–æ—Ñ–µ\", \"–ß–∞—à–∫–∞ –ö–æ—Ñ–µ\"]</td>\n",
       "      <td>publish</td>\n",
       "      <td>[31495]</td>\n",
       "      <td>Tolmachevo-MTS-Free</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"type\": \"mining\", \"value\": \"https://2gis.ru/...</td>\n",
       "      <td>55.007579</td>\n",
       "      <td>82.666723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–†–æ—Å—Å–∏—è, –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –û–±—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –ú...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>55.007337</td>\n",
       "      <td>82.667066</td>\n",
       "      <td>[\"–ß–∞—à–∫–∞ –∫–æ—Ñ–µ\", \"Chashka kofe\", \"–ß–∞—à–∫–∞ –∫–æ—Ñ–µ\", \"...</td>\n",
       "      <td>publish</td>\n",
       "      <td>[31495]</td>\n",
       "      <td>Tolmachevo-MTS-Free</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"type\": \"main\", \"value\": \"http://chashkacoff...</td>\n",
       "      <td>55.007579</td>\n",
       "      <td>82.666723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–†–æ—Å—Å–∏—è, –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –û–±—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –ú...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.007623</td>\n",
       "      <td>82.666341</td>\n",
       "      <td>[\"–¢–µ–ª–µ—Ñ–æ–Ω –¥–æ–≤–µ—Ä–∏—è\"]</td>\n",
       "      <td>obsolete</td>\n",
       "      <td>[30078]</td>\n",
       "      <td>Tolmachevo-MTS-Free</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"type\": \"mining\", \"value\": \"https://2gis.ru/...</td>\n",
       "      <td>55.007579</td>\n",
       "      <td>82.666723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             address  candidate_num  \\\n",
       "0  –†–æ—Å—Å–∏—è, –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –û–±—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –ú...              0   \n",
       "1  –†–æ—Å—Å–∏—è, –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –∞—ç—Ä–æ–ø–æ—Ä—Ç –ù–æ–≤–æ—Å–∏...              1   \n",
       "2  –†–æ—Å—Å–∏—è, –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –û–±—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –ú...              2   \n",
       "3  –†–æ—Å—Å–∏—è, –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –û–±—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –ú...              3   \n",
       "4  –†–æ—Å—Å–∏—è, –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –û–±—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –ú...              4   \n",
       "\n",
       "   group_num has_wifi        lat        lon  \\\n",
       "0          0      NaN  55.007759  82.667023   \n",
       "1          0      NaN  55.007879  82.665401   \n",
       "2          0      NaN  55.007570  82.667069   \n",
       "3          0     True  55.007337  82.667066   \n",
       "4          0      NaN  55.007623  82.666341   \n",
       "\n",
       "                                               names publishing_status  \\\n",
       "0  [\"–°–±–µ—Ä–±–∞–Ω–∫ –†–æ—Å—Å–∏–∏, –±–∞–Ω–∫–æ–º–∞—Ç\", \"Sberbank Rossii...           publish   \n",
       "1                                          [\"–ò–ª-86\"]           publish   \n",
       "2                       [\"–ß–∞—à–∫–∞ –∫–æ—Ñ–µ\", \"–ß–∞—à–∫–∞ –ö–æ—Ñ–µ\"]           publish   \n",
       "3  [\"–ß–∞—à–∫–∞ –∫–æ—Ñ–µ\", \"Chashka kofe\", \"–ß–∞—à–∫–∞ –∫–æ—Ñ–µ\", \"...           publish   \n",
       "4                                [\"–¢–µ–ª–µ—Ñ–æ–Ω –¥–æ–≤–µ—Ä–∏—è\"]          obsolete   \n",
       "\n",
       "        rubrics                 ssid  target  \\\n",
       "0       [30336]  Tolmachevo-MTS-Free       0   \n",
       "1  [3481524327]  Tolmachevo-MTS-Free       0   \n",
       "2       [31495]  Tolmachevo-MTS-Free       0   \n",
       "3       [31495]  Tolmachevo-MTS-Free       0   \n",
       "4       [30078]  Tolmachevo-MTS-Free       0   \n",
       "\n",
       "                                                urls   user_lat   user_lon  \n",
       "0  [{\"type\": \"main\", \"value\": \"http://www.sberban...  55.007579  82.666723  \n",
       "1                                                 []  55.007579  82.666723  \n",
       "2  [{\"type\": \"mining\", \"value\": \"https://2gis.ru/...  55.007579  82.666723  \n",
       "3  [{\"type\": \"main\", \"value\": \"http://chashkacoff...  55.007579  82.666723  \n",
       "4  [{\"type\": \"mining\", \"value\": \"https://2gis.ru/...  55.007579  82.666723  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('input/l_train.tsv', sep='\\t', index_col=0)\n",
    "df_test = pd.read_csv('input/l_test.tsv', sep='\\t', index_col=0)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter rows with **target==1** and look at **ssid** and **names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>ssid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[\"–ê—ç—Ä–æ–ø–æ—Ä—Ç –¢–æ–ª–º–∞—á–µ–≤–æ, –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏—è\", \"–¢–æ–ª–º–∞—á–µ–≤–æ\"]</td>\n",
       "      <td>Tolmachevo-MTS-Free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[\"Kontrolmatik\"]</td>\n",
       "      <td>Kontrolmatik_Staff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[\"–ü–ö–í –ú–æ—Ç–æ—Ä—Å\", \"Pkw Motors\", \"Pkw Motors\", \"–¢–µ...</td>\n",
       "      <td>PKW Guests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>[\"–¢–µ—Ö—Ü–µ–Ω—Ç—Ä –Æ—Å—Ç–∞\", \"Tekhtsentr Yusta\", \"–Æ—Å—Ç–∞\", ...</td>\n",
       "      <td>YUSTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>[\"–†–µ—Å–ø–µ–∫—Ç –ê–≤—Ç–æ\", \"–ê–≤—Ç–æ—Å–µ—Ä–≤–∏—Å\"]</td>\n",
       "      <td>RespectAuto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                names                 ssid\n",
       "18   [\"–ê—ç—Ä–æ–ø–æ—Ä—Ç –¢–æ–ª–º–∞—á–µ–≤–æ, –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏—è\", \"–¢–æ–ª–º–∞—á–µ–≤–æ\"]  Tolmachevo-MTS-Free\n",
       "38                                   [\"Kontrolmatik\"]   Kontrolmatik_Staff\n",
       "49  [\"–ü–ö–í –ú–æ—Ç–æ—Ä—Å\", \"Pkw Motors\", \"Pkw Motors\", \"–¢–µ...           PKW Guests\n",
       "77  [\"–¢–µ—Ö—Ü–µ–Ω—Ç—Ä –Æ—Å—Ç–∞\", \"Tekhtsentr Yusta\", \"–Æ—Å—Ç–∞\", ...                YUSTA\n",
       "94                     [\"–†–µ—Å–ø–µ–∫—Ç –ê–≤—Ç–æ\", \"–ê–≤—Ç–æ—Å–µ—Ä–≤–∏—Å\"]          RespectAuto"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.target == 1][['names', 'ssid']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice, that almost always they have a lot in common. Though sometimes they are written in different\n",
    "languages/cases. A simple heuristic to meausure how much strings have in common would be to transliterate\n",
    "cyrillyc and accented letters and to calculate how many char ngrams they have in common for different **n**. I used **n=1..8**. That are very strong features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute features. It might take up to 7 minutes and several GB of RAM. The longset part is one-hot encoding rubrics and calculating number of matching ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109016, 1210)\n",
      "CPU times: user 6min 24s, sys: 27.3 s, total: 6min 51s\n",
      "Wall time: 6min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.concat((df_train, df_test))\n",
    "df['test'] = df.target.isnull()\n",
    "df['publishing_status'] = df['publishing_status'].astype('category')\n",
    "df['has_wifi'] = df['has_wifi'].astype(float)\n",
    "df['distance_km'] = haversine_np(df.lon, df.lat, df.user_lon, df.user_lat)\n",
    "df['distance_lat'] = df.user_lat - df.lat\n",
    "df['distance_lon'] = df.user_lon - df.lon\n",
    "df['abs_distance_lat'] = abs(df.user_lat - df.lat)\n",
    "df['abs_distance_lon'] = abs(df.user_lon - df.lon)\n",
    "df = pd.concat((df, df.rubrics.str.replace('[\\[\\]\\s]', '').str.get_dummies(',').add_prefix('rubric_')), axis=1)\n",
    "for i in range(1, 9):\n",
    "    df[f'ngrams_match_names_{i}'] = df.apply(lambda x: compare_ngrams(unidecode(x.names.lower()), unidecode(x.ssid.lower()), i), axis=1)\n",
    "    df[f'ngrams_match_urls_{i}'] = df.apply(lambda x: compare_ngrams(unidecode(x.urls.lower()), unidecode(x.ssid.lower()), i), axis=1)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should use group-aware validation, so I used `GroupShuffleSplit` which will place all samples from one group\n",
    "to either train or test set, so that test and train sets will have non-intersecting groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['lon', 'lat', 'has_wifi', 'user_lon', 'user_lat', 'publishing_status', 'distance_km',\n",
    "          'distance_lat', 'distance_lon', 'abs_distance_lat', 'abs_distance_lon'] + \\\n",
    "          [c for c in df.columns if c.startswith('rubric_')] + \\\n",
    "          [c for c in df.columns if c.startswith('ngrams_match_')]\n",
    "train_idx, test_idx = next(GroupShuffleSplit(1, test_size=0.20, random_state=2)\n",
    "                           .split(df[~df.test].index, groups=df[~df.test].group_num))\n",
    "train_ds = lgb.Dataset(df.iloc[train_idx][columns],\n",
    "                       df.iloc[train_idx].target, params={'train': 1})\n",
    "valid_ds = lgb.Dataset(df.iloc[test_idx][columns],\n",
    "                       df.iloc[test_idx].target, params={'train': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\ttraining's binary_logloss: 0.110857\ttraining's score: 3.54714\tvalid_1's binary_logloss: 0.112376\tvalid_1's score: 3.48571\n",
      "[40]\ttraining's binary_logloss: 0.0525294\ttraining's score: 3.63571\tvalid_1's binary_logloss: 0.0575634\tvalid_1's score: 3.54286\n",
      "[60]\ttraining's binary_logloss: 0.0402884\ttraining's score: 3.72714\tvalid_1's binary_logloss: 0.0493212\tvalid_1's score: 3.58286\n",
      "[80]\ttraining's binary_logloss: 0.0344945\ttraining's score: 3.77429\tvalid_1's binary_logloss: 0.0472072\tvalid_1's score: 3.58286\n",
      "[100]\ttraining's binary_logloss: 0.0304829\ttraining's score: 3.82571\tvalid_1's binary_logloss: 0.0463274\tvalid_1's score: 3.59429\n",
      "[120]\ttraining's binary_logloss: 0.0276586\ttraining's score: 3.85429\tvalid_1's binary_logloss: 0.0456935\tvalid_1's score: 3.6\n",
      "[140]\ttraining's binary_logloss: 0.0251468\ttraining's score: 3.89\tvalid_1's binary_logloss: 0.0454254\tvalid_1's score: 3.61714\n",
      "[160]\ttraining's binary_logloss: 0.0228951\ttraining's score: 3.90714\tvalid_1's binary_logloss: 0.0452949\tvalid_1's score: 3.58857\n",
      "[180]\ttraining's binary_logloss: 0.0211293\ttraining's score: 3.92571\tvalid_1's binary_logloss: 0.0452031\tvalid_1's score: 3.57143\n",
      "[200]\ttraining's binary_logloss: 0.0195177\ttraining's score: 3.94143\tvalid_1's binary_logloss: 0.0450583\tvalid_1's score: 3.55429\n",
      "[220]\ttraining's binary_logloss: 0.0180155\ttraining's score: 3.95571\tvalid_1's binary_logloss: 0.0451257\tvalid_1's score: 3.57143\n",
      "[240]\ttraining's binary_logloss: 0.016726\ttraining's score: 3.96857\tvalid_1's binary_logloss: 0.0450745\tvalid_1's score: 3.54286\n",
      "[260]\ttraining's binary_logloss: 0.0154913\ttraining's score: 3.98\tvalid_1's binary_logloss: 0.0451456\tvalid_1's score: 3.55429\n",
      "[280]\ttraining's binary_logloss: 0.0143795\ttraining's score: 3.98571\tvalid_1's binary_logloss: 0.0452139\tvalid_1's score: 3.58286\n",
      "[300]\ttraining's binary_logloss: 0.0134922\ttraining's score: 3.98571\tvalid_1's binary_logloss: 0.0452774\tvalid_1's score: 3.57714\n"
     ]
    }
   ],
   "source": [
    "params = { 'objective': 'binary' }\n",
    "model = lgb.train(params, train_ds, valid_sets=[train_ds, valid_ds], verbose_eval=20, num_boost_round=300, feval=compute_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we see that validation score stops increasing quite soon after **140** iterations reaching **3.61** out of 4 which is not so bad üòé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we know how much rounds we need, we can feed the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = lgb.Dataset(df[~df.test][columns], df[~df.test].target, params={'train': 1})\n",
    "params = { 'objective': 'binary' }\n",
    "train_idx = df[~df.test].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\ttraining's binary_logloss: 0.232019\ttraining's score: 3.43657\n",
      "[20]\ttraining's binary_logloss: 0.111026\ttraining's score: 3.54971\n",
      "[30]\ttraining's binary_logloss: 0.0690424\ttraining's score: 3.61486\n",
      "[40]\ttraining's binary_logloss: 0.0528524\ttraining's score: 3.64114\n",
      "[50]\ttraining's binary_logloss: 0.0454717\ttraining's score: 3.67543\n",
      "[60]\ttraining's binary_logloss: 0.0413087\ttraining's score: 3.70514\n",
      "[70]\ttraining's binary_logloss: 0.0382407\ttraining's score: 3.73829\n",
      "[80]\ttraining's binary_logloss: 0.036007\ttraining's score: 3.74857\n",
      "[90]\ttraining's binary_logloss: 0.0341142\ttraining's score: 3.77143\n",
      "[100]\ttraining's binary_logloss: 0.0324071\ttraining's score: 3.79543\n",
      "[110]\ttraining's binary_logloss: 0.0310263\ttraining's score: 3.80686\n",
      "[120]\ttraining's binary_logloss: 0.0297982\ttraining's score: 3.824\n",
      "[130]\ttraining's binary_logloss: 0.0286451\ttraining's score: 3.83657\n",
      "[140]\ttraining's binary_logloss: 0.0276137\ttraining's score: 3.84343\n",
      "[150]\ttraining's binary_logloss: 0.0266454\ttraining's score: 3.85943\n",
      "[160]\ttraining's binary_logloss: 0.0256527\ttraining's score: 3.87086\n",
      "[170]\ttraining's binary_logloss: 0.0247839\ttraining's score: 3.88229\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(params, train_ds, valid_sets=train_ds, verbose_eval=10, num_boost_round=170, feval=compute_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict, save and submit! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(df[df.test][columns])\n",
    "predictions = pd.concat((df[df.test], pd.Series(predictions, name='pred')), axis=1)[['group_num', 'pred']]\n",
    "predictions['target'] = 0\n",
    "predictions.loc[predictions[['group_num', 'pred']].groupby('group_num').idxmax().values[:, 0], 'target'] = 1\n",
    "predictions.target.to_csv('output/l.out', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
